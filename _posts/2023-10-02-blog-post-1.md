---
title: 'Top Papers of the Week (Sep 25 - Oct 1)'
date: 2023-10-02
permalink: /posts/2012/08/blog-post-1/
tags:
  - Top ML Papers
  - large language models
  - NLP
---

In this blog post I tried to introduce top papers in ML (particularly in NLP) and give a brief overview on them.

Paper #1: ADAPTING LARGE LANGUAGE MODELS VIA READING COMPREHENSION
======
overview 1

Paper #2: Graph Neural Prompting with Large Language Models
======
![Alt Text](/images/GNP.png)
### Motivation
Despite the success of Large Language Models (LLMs) in handling different real-world applications and availablity of aligning these LLMs to different downstream tasks, they have been denounced for memorizing facts and knowledge (capturing and returning knowledge properly). Knowledge Graphs (KGs), on the other hand, offer a structured and systematic approach to knowledge representation. As a result exsisting methods have emerged, aiming to combine KGs and textual data during the pre-training of LLMs. Nevertheless, integrating KGs and text during pre-training can be challenging due to the **extensive parameters LLMs contain**. Therefore **one primary motivation** here is to **employ pre-existing pre-trained LLMs**. A straightforward approach would be to feed KG triples directly into LLMs. However, KGs often contain extraneous contextual information that can mislead LLMs. This issue gives rise to **another significant motivation** for this work: to **identify beneficial knowledge from KGs and integrate them into pre-trained LLMs**.To address these challenges, authors propose **Graph Neural Prompting** (GNP) a novel plug-and-play method to assist LLMs in learning beneficial knowledge from KGs.

### Contributions
In this paper, the authors introduce GNP as a method to learn and extract the most valuable knowledge from KGs to enrich pre-trained LLMs. The key contribution of this work is its ability to improve baseline LLM performance by 13.5% when the LLM is kept frozen. In addition, the proposed method contains various tailored designs, including a standard **GNN**, a **cross-modality pooling module**, a **domain projector**, and a **self-supervised graph learning objective**.

### Methodology
It needs to be mentioned that the assumed task is **multiple choice questions answering**.

#### Prompting LLMs for Question Answering
Prompting is the defacto approach to elicit respones from LLMs. To create prompts for multiple choice question answering given questions $Q$, the optional context $C$, and the answer options $A$:
- First tokenize the concatenation of $C$, $Q$, $A$ into a sequence of text tokens $X$.
- Then designs a series of prompt tokens &P&
- Finally Prepend &p& to &X& tokens which will be the input to the LLM.
The LLM model can be trained with an objective of **maximum likelihood of cross-entropy loss**:
$$\mathcal{L}_{llm} = - \log p(y|X, \Theta)$$

where $\Theta$ refers to the parameters of the model.

#### Subgraph Retrieval
To align the input text tokens $X$ with the massive knowledge graph $G$, we retrieve subgraph of $G^ \prime$ containing relevant entities to the tokens in $X$. In this stage the retrieval happens based on two-hop neighbors.

#### Graph Neural Prompting
![Alt Text](/images/GNP-Module.png)
Graph Neural Prompting is composed of four different modules:
- **GNN Encoder**
- **Cross-modality Pooling Module**
- **Domain Projector**
- **Self-supervised Link Prediction**

##### GNN Encoder
To avoid knowledge noises, we employ a graph neural network (GNN) to encode the knowledge inside the subgraph $G^ \prime$. To do so, first we initialize the node embeddings using pre-trained entity embeddings (from prior works on entity embeddings) and then a standard graph attention network [(GAT)](https://arxiv.org/abs/1710.10903) is employed as our GNN to encode the most relevant entities and their complex relationships.

![GAT](/images/GAT.png)
<figcaption>An illustration of multihead attention (with K = 3 heads) by node 1 on its neighborhood. Different arrow styles and colors denote independent attention computations. The final value $h^ \prime$ indicates the encoded entity containing the most relevant knowledge from its neighbors.</figcaption>

The encoding process is formulated as follows:
$$H_1 = \mathcal{f}{GNN}(G^ \prime)$$

Where $H_1$ represents the node embeddings learned by GNN.

##### Cross-modality Pooling

Paper #3: Language Modeling Is Compression
======
overview 3