---
title: 'Top Papers of the Week (Sep 25 - Oct 1)'
date: 2023-10-02
permalink: /posts/2012/08/blog-post-1/
tags:
  - Top ML Papers
  - large language models
  - NLP
---

In this blog post I tried to introduce top papers in ML (particularly in NLP) and give a brief overview on them.

Paper #1: ADAPTING LARGE LANGUAGE MODELS VIA READING COMPREHENSION
======
overview 1

Paper #2: Graph Neural Prompting with Large Language Models
======
![Local Image](./images/GNP.png)
### Motivation
Despite the success of Large Language Models (LLMs) in handling different real-world applications and availablity of aligning these LLMs to different downstream tasks, they have been denounced for memorizing facts and knowledge (capturing and returning knowledge properly). Knowledge Graphs (KGs), on the other hand, offer a structured and systematic approach to knowledge representation. As a result exsisting methods have emerged, aiming to combine KGs and textual data during the pre-training of LLMs. Nevertheless, integrating KGs and text during pre-training can be challenging due to the **extensive parameters LLMs contain**. Therefore **one primary motivation** here is to **employ pre-existing pre-trained LLMs**. A straightforward approach would be to feed KG triples directly into LLMs. However, KGs often contain extraneous contextual information that can mislead LLMs. This issue gives rise to **another significant motivation** for this work: to **identify beneficial knowledge from KGs and integrate them into pre-trained LLMs**.To address these challenges, authors propose **Graph Neural Prompting** (GNP) a novel plug-and-play method to assist LLMs in learning beneficial knowledge from KGs.

### Contributions
In this paper, the authors introduce GNP as a method to learn and extract the most valuable knowledge from KGs to enrich pre-trained LLMs. The key contribution of this work is its ability to improve baseline LLM performance by 13.5% when the LLM is kept frozen. In addition, the proposed method contains various tailored designs, including a standard **GNN**, a **cross-modality pooling module**, a **domain projector**, and a **self-supervised graph learning objective**.

### Methodology
It needs to be mentioned that the assumed task is **multiple choice questions answering**.

#### Prompting LLMs for Question Answering
Prompting is the defacto approach to elicit respones from LLMs. To create prompts for multiple choice question answering given questions $Q$, the optional context $C$, and the answer options $A$:
- First tokenize the concatenation of $C$, $Q$, $A$ into a sequence of text tokens $X$.
- Then designs a series of prompt tokens &P&
- Finally Prepend &p& to &X& tokens which will be the input to the LLM.
The LLM model can be trained with an objective of **maximum likelihood of cross-entropy loss**:



Paper #3: Language Modeling Is Compression
======
overview 3