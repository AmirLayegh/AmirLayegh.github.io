---
title: 'Top Papers of the Week (Sep 25 - Oct 1)'
date: 2023-10-02
permalink: /posts/2012/08/blog-post-1/
tags:
  - Top ML Papers
  - large language models
  - NLP
---

In this blog post I tried to introduce top papers in ML (particularly in NLP) and give a brief overview on them.

Paper #1: [Effective Long-Context Scaling of Foundation Models](https://arxiv.org/abs/2309.16039)
======
![Long-context Llama 2](/images/long-context-llama.png)
<figcaption>We show that our modelâ€™s validation loss can be fit as a function of the context length. This power-law relationship also suggests that context length is another important axis of scaling LLMs and our model can continually improve its performance as we increase the context length up to 32,768 tokens.</figcaption>

## Motivation
The motivation behind this paper stems from the need to develop and enhance language models capable of handling significantly longer contexts. The authors introduce a series of long-context Large Language Models (LLMs) designed to process context windows of up to 32,768 tokens effectively.

## Contributions
The paper presents a significant contribution in two aspects. Firstly, it reveals a robust power-law scaling pattern in language models, emphasizing the importance of extended context for Large Language Models (LLMs). This scaling behavior enhances the ability of LLMs to effectively utilize more extensive contexts. Secondly, when compared to LLAMA 2 on research benchmarks, the models exhibit substantial improvements in long-context tasks and modest enhancements in standard short-context tasks, particularly in coding, math, and knowledge benchmarks. Furthermore, the paper introduces a cost-effective procedure for instruction finetuning in continually pretrained long models **without any human-annotated data**, resulting in a chat model that outperforms gpt-3.5-turbo-16k on various long-context benchmarks, including question answering, summarization, and multi-document aggregation tasks.

## Methodology











Paper #2: [Graph Neural Prompting with Large Language Models](https://arxiv.org/abs/2309.15427#:~:text=Large%20Language%20Models%20(LLMs)%20have,capturing%20and%20returning%20grounded%20knowledge.)
======
![Alt Text](/images/GNP.png)
<figcaption>The overview of GNP</figcaption>

## Motivation
Despite the success of Large Language Models (LLMs) in handling different real-world applications and availablity of aligning these LLMs to different downstream tasks, they have been denounced for memorizing facts and knowledge (capturing and returning knowledge properly). Knowledge Graphs (KGs), on the other hand, offer a structured and systematic approach to knowledge representation. As a result exsisting methods have emerged, aiming to combine KGs and textual data during the pre-training of LLMs. Nevertheless, integrating KGs and text during pre-training can be challenging due to the **extensive parameters LLMs contain**. Therefore **one primary motivation** here is to **employ pre-existing pre-trained LLMs**. A straightforward approach would be to feed KG triples directly into LLMs. However, KGs often contain extraneous contextual information that can mislead LLMs. This issue gives rise to **another significant motivation** for this work: to **identify beneficial knowledge from KGs and integrate them into pre-trained LLMs**.To address these challenges, authors propose **Graph Neural Prompting** (GNP) a novel plug-and-play method to assist LLMs in learning beneficial knowledge from KGs.

## Contributions
In this paper, the authors introduce GNP as a method to learn and extract the most valuable knowledge from KGs to enrich pre-trained LLMs. The key contribution of this work is its ability to improve baseline LLM performance by 13.5% when the LLM is kept frozen. In addition, the proposed method contains various tailored designs, including a standard **GNN**, a **cross-modality pooling module**, a **domain projector**, and a **self-supervised graph learning objective**.

## Methodology
It needs to be mentioned that the assumed task is **multiple choice questions answering**.

### Prompting LLMs for Question Answering
Prompting is the defacto approach to elicit respones from LLMs. To create prompts for multiple choice question answering given questions $Q$, the optional context $C$, and the answer options $A$:
- First tokenize the concatenation of $C$, $Q$, $A$ into a sequence of text tokens $X$.
- Then designs a series of prompt tokens &P&
- Finally Prepend &p& to &X& tokens which will be the input to the LLM.
The LLM model can be trained with an objective of **maximum likelihood of cross-entropy loss**:
$$\mathcal{L}_{llm} = - \log p(y|X, \Theta)$$

where $\Theta$ refers to the parameters of the model.

### Subgraph Retrieval
To align the input text tokens $X$ with the massive knowledge graph $G$, we retrieve subgraph of $G^ \prime$ containing relevant entities to the tokens in $X$. In this stage the retrieval happens based on two-hop neighbors.

### Graph Neural Prompting
![Alt Text](/images/GNP-Module.png)
<figcaption> Illusteration of different modules of GNP</figcaption>
Graph Neural Prompting is composed of four different modules:
- **GNN Encoder**
- **Cross-modality Pooling Module**
- **Domain Projector**
- **Self-supervised Link Prediction**

#### GNN Encoder
To avoid knowledge noises, we employ a graph neural network (GNN) to encode the knowledge inside the subgraph $G^ \prime$. To do so, first we initialize the node embeddings using pre-trained entity embeddings (from prior works on entity embeddings) and then a standard graph attention network [(GAT)](https://arxiv.org/abs/1710.10903) is employed as our GNN to encode the most relevant entities and their complex relationships.

![GAT](/images/GAT.png)
<figcaption>An illustration of multihead attention (with K = 3 heads) by node 1 on its neighborhood. Different arrow styles and colors denote independent attention computations. The final value $h^ \prime$ indicates the encoded entity containing the most relevant knowledge from its neighbors.</figcaption>

The encoding process is formulated as follows:

$$H_1 = \mathcal{f}_{GNN}(G^ \prime)$$

Where $H_1$ represents the node embeddings learned by GNN.

#### Cross-modality Pooling
The main objective of this module is to identify the most relevant entities of encoded subgraph based on the prompt ontext (input tokens $X$). This identification occurs through a series of distinct stages.

In the initial step, a self-attention layer is employed on the node embeddings ($H_1$), yielding:

$$H_2 = Self-Attn(H_1)$$ 

In this equation, $H_2$ represents the node embeddings subsequent to the application of self-attention.

The second step involves transformation of the embedding of input tokens ($X$) into the space of node embeddings $H_2$, achieved through the following operation:

$$\mathcal{T^ \prime} = FFN_1(\sigma(FFN_2(\mathcal{T})))$$ 

Here, $\mathcal{T}^ \prime$ signifies the embedding of input tokens within the dimensional space of node embeddings, and $\mathcal{T}$ denotes the initial representation of input tokens. The $\sigma$ idenotes the GELU activation function, and $FFN_1$ and $FFN_2$ correspond to feed-forward networks.

Subsequently, cross-modality attention is computed using $H_2$ as the query and $\mathcal{T}^ \prime$ as the key and value:

$$H_3 = softmax[H_2.(\mathcal{T^ \prime})^T / \sqrt(d_g)].\mathcal{T^ \prime}$$

In the above formula, $H_3$ represents the final node embeddings, achieved through the utilization of cross-modality attention.

a graph-level embedding is generated by performing average pooling on the node embeddings ($H_3$):

$$H_4 = POOL(H_3)$$

#### Domain Projector
To establish a robust mapping between the graph-level embedding and the input text embedding, a domain projector is employed. This projector serves the crucial purpose of bridging the inherent disparities between graph-level node embeddings and input text embeddings, facilitating their seamless integration. In essence, the projector transforms the graph-level embeddings into the same dimension as the input text embeddings ($d_t$).

This transformation is achieved through the following operation:

$$Z = FFN_3(\sigma(FFN_4(H_4)))$$

Here, $Z$ denotes the **Graph Neural Prompt**, and $FFN_3$ and $FFN_4$ are feed-forwar neural networks. The Domain Projection process acts as a pivotal link, ensuring that graph-level embeddings are seamlessly integrated into the input text embedding space, thereby enhancing the model's overall understanding and performance.

#### Self-supervised Link Prediction
While the cross-entropy objective enables the model to learn and adapt to the target dataset, a link prediction task is introduced to further refine the model's understanding of relationships between entities. This task involves masking out a set of edges ($\varepsilon_{mask}$) within the subgraph $G^{\prime}$. Given the node embeddings of both the head entity and the tail entity in a triplet, denoted as ${h_i, t_i} \in H_3$, a method called DistMult is utilized to map these embeddings into vectors, specifically $h, r, t$. A score function, $\phi(h_i, t_i) = \langle h, t, r \rangle$, is defined to generate scores for each triplet. A higher $\phi$ value indicates a higher likelihood that ${h, t, r}$ represents a correct positive triple rather than an incorrect negative triple.

In this context, the model is tasked with predicting the masked edges ($\varepsilon_{mask}$) as positives while treating other randomly selected edges as negatives. The link prediction loss is formally defined as follows:

$$\mathcal{L}_{lp} = \sum_{(h_i, r, t_i) \in \varepsilon_{mask}}^{} (\mathcal{S}_{pos} + \mathcal{S}_{neg})$$

The final objective funtion $\mathcal{L}$ is defined as the weighted combination of $\mathcal{L}$_{llm} and $\mathcal{L}$_{lp}:

$$\mathcal{L} = \mathcal{L}_{llm} + \lambda \mathcal{L}_{lp}$$

Incorporating self-supervised link prediction enhances the model's grasp of entity relationships, contributing to a more refined and comprehensive understanding of the data.

## Strong Points of the paper
- This approach demonstrates a substantial performance improvement, with a +13.5% enhancement in the baseline when the LLM is kept frozen.
- The GNP method presents a novel solution for effectively filtering out noise from knowledge graphs and integrating the valuable insights they contain into the prompt.

## Weak Points of the Paper
- This approach is only applied on multiple choice question answering task.
- The choice of Falcon-T5 as the underlying LLM is not accompanied by a clear rationale. Several other LLMs, like Llama 2, could be viable alternatives for these tasks.
- The paper lacks an in-depth exploration of the complexities involved in both training and inference processes, particularly with regard to the time requirements. A more comprehensive discussion of these aspects would provide valuable insights into the method's practical feasibility.

Paper #3: Language Modeling Is Compression
======
overview 3