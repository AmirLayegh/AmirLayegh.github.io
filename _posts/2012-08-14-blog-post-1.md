---
title: 'Top Papers of the Week (Sep 25 - Oct 1)'
date: 2023-10-02
permalink: /posts/2012/08/blog-post-1/
tags:
  - Top ML Papers
  - large language models
  - NLP
---

In this blog post I tried to introduce top papers in ML (particularly in NLP) and give a brief overview on them.

Paper #1: ADAPTING LARGE LANGUAGE MODELS VIA READING COMPREHENSION
======
overview 1

Paper #2: Graph Neural Prompting with Large Language Models
======
#### Motivation
Despite the success of Large Language Models (LLMs) in handling different real-world applications and availablity of aligning these LLMs to different downstream tasks, they have been denounced for memorizing facts and knowledge (capturing and returning knowledge properly). On the other hand, Knowledge Graphs (KGs) are the source of structured and systematic way for knowledge representation. As a result exsisting methods have incorporated architectures using joint pre-training sessions to utilize both KGs and taxtual data to enhance language modeling. This process results in new pre-trained LLMs. Nonetheles, joint pre-training KGs and text for LLMs might be challenging due to **extensive parameters LLMs contain**. Therefore **one motivation** here is to **employ pre-existing pre-trained LLMs**. One direct approach to employing KGs is to feed the triples from KGs into LLMs directly. However, given that KGs might contain various extraneous context, the knowledge noise can mislead the LLMs. Consequently, **another motivation** here is to **learn beneficial knowledge from KGs and integrate them into pre-trained LLMs**. As a result the author propose **Graph Neural Prompting** (GNP) a novel plug-and-play method to assist LLMs in learning beneficial knowledge from KGs.

### Contributions

Paper #3: Language Modeling Is Compression
======
overview 3