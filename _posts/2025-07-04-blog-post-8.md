---
title: 'From Local to Global Sense-Making: First Impressions of GraphRAG'
date: 2025-07-04
permalink: /posts/2025/04/blog-post-8/
tags:
  - Retrieval Augmented Generation
  - GraphRAG
  - Query-Focused Summarization
---

**TL;DR**
GraphRAG replaces vector search with a lightweight knowledge-graph index and a map-reduce summarization step.
The result: LLMs can tackle global questions such as "What themes span this entire corpus?" while remaining fast and token-efficient. In head-to-head tests against GPT-4-powered vector RAG, GraphRAG won 72-83 % of comparisons on answer comprehensiveness and 62-82 % on diversity, while using up to 97 % fewer context tokens for some query modes. 

<figure style="text-align: center; margin: 20px 0;">
  <img src="/assets/use_cases/graphrag/GraphRag-Figure1.jpg" alt="Knowledge Graph" style="width: 80%; max-width: 800px;">
  <figcaption>Figure 1: An LLM-generated knowledge graph built using GPT-4 Turbo and Microsoft GraphRAG</figcaption>
</figure>

# What is Naive (Vector) RAG?
Retrieval Augmented Generation (RAG) refers to a system where a user query 
is used to retireve most relevant information from external data sources. 
These relevant information will be provided to an LLM to use and generate 
the answer based on the user query. Essentially,  Naive RAG follows a two-stage approach: **Retrieval** and **Generation**.

<figure style="text-align: center; margin: 20px 0;">
  <img src="/assets/use_cases/graphrag/NaiveRAG.png" alt="Naive RAG Workflow" style="width: 90%; max-width: 900px;">
  <figcaption>Figure 2: Traditional RAG workflow showing retrieval and generation stages</figcaption>
</figure>

**Retrieval Stage** consists of two phases:
- **Indexing Phase**: Documents are split into chunks, converted into vector embeddings using an embedding model, and stored in a vector index. This creates a searchable knowledge base where semantically similar content clusters together in the vector space.
- **Search Phase**: When a user asks a question, the query is embedded using the same model, and the system performs nearest neighbor search to find the most semantically similar document chunks. These are then ranked by similarity score to produce the top-k results.

**Generation Stage**: The retrieved top-k results are combined with the user query in a prompt template and fed to the language model to generate the final answer.

This approach works well for **local** questions where the answer can be found in a few relevant chunks. However, it struggles with **global** reasoning tasks that require understanding connections and themes across the entire corpus.

# Why Does Classic Naive RAG Break Down?
Naive RAG excels when answers exist in a handful of chunks that fit the model's context window. Global "sensemaking" questions, however, require reasoning across the entire corpus rather than just the top-k nearest neighbours. Sensemaking tasks require reasoning over **connections** which can be among entities (persons, places) and relations between them. A query like "What overarching themes emerge across the entire dataset?" makes this clear. 

To address this limitation, [Microsoft proposed and implemented GraphRAG](https://microsoft.github.io/graphrag/) which has become one of the most popular approaches. In this article, we will dive into how GraphRAG works, explore its key innovations, and see why it is proving so effective for global reasoning tasks.


GraphRAG reframes such global queries as query-focused summarization (QFS) tasks and supplies the model with pre-digested, graph-structured knowledge instead of raw text. 

