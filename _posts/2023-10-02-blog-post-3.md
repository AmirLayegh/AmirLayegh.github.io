---
title: 'Prompt Construction Guide'
date: 2023-10-09
permalink: /posts/2023/10/blog-post-3/
output: html_document
bibliography: refs.bib  
tags:
  - Prompt Construction
  - Prompt Engineering
  - Large Language Models
---

Prompt Engineering
======
Prompt engineering is a discipline focused on crafting efficient prompts for large language models (LLMs). These skills aid in comprehending LLM capabilities and limitations. Researchers apply prompt engineering to enhance LLM performance in various tasks, from QA to arithmetic reasoning. Developers use it to create compelling interfaces for LLMs and other tools.

Beyond prompt design, prompt engineering encompasses a spectrum of skills essential for LLM interaction and development. Prompt engineering is a crucial competency for understanding and enhancing LLM capabilities, improving safety, and incorporating domain knowledge and external tools.

This guide compiles the latest papers, and models for prompt engineering to meet the growing interest in LLM development. In this blog post, I delve into the art of prompt engineering for LLMs, exploring a variety of strategies and techniques to craft effective prompts that harness the full potential of these robust AI systems. Whether you are a developer, researcher, or enthusiast, this resource will empower you to create prompts that yield insightful and contextually relevant responses.

In this guide, we divide the prompts into two distinct settings: 
- [Fill-in Slot Prompts](#fill-in-slot-prompts-for-prompt-based-learning)
- [Prompts for Generative Large Language Models](#prompts-for-generative-large-language-models) 

This division helps us explore specialized techniques and best practices for each type of prompt construction, ensuring that you have a comprehensive understanding of how to effectively engineer prompts for both scenarios.

## Fill-in Slot Prompts for Prompt-based Learning
In contrast to traditional supervised learning, which trains a model to take an input $x$ and predict an output $y$ as $P(y|x)$, **prompt-based learning** relies on language models that directly model the probability of text[^1]. In this paradigm, a textual prompt template with unfilled slots is applied to an input sentence $x$, transforming the downstream task into a Masked Language Modeling (MLM) problem. This reformulation empowers the language model to statistically fill in the unfilled slots, which can be considered as the predicted label $y$ for the specific task.
For instance the prompt template $T$ for a sentiment classification task can be defined as follow:
```
T = x. The sentiment of the sentence is [MASK].
```
where $x$ indicates the original input sentence.

In case of having an input sentence $x$ = *That is an awesome product.*, applying the prompt template $T$ on the input sentence $x$, we will have the prompted input sentence $x^ \prime$ as:
```
xÂ´ = That is an awesome product. The sentiment of the sentence is [MASK].
```

Given the prompted input sentnece $x ^\prime$, the language model will fill the unfilled mask token [MASK], which will be translated into one of the actual labels $y$ by a verbalizer (See Figure 1).

![Prompt learning](/images/prompt.png)
<figcaption> Figure 1: The example of prompt-based learning for sentiment analysis.</figcaption>

## Prompts for Generative Large Language Models

![](/images/Prompt-LLM.png)
<figcaption> Figure 2: Example of prompting LLMs on sentiment analysis task.</figcaption>

LLMs, such as GPT-3[^2] and Llama 2[^3], are usually very good at generating grammatically correct and semantically meaningful text. However, despite their outstanding performance, they can produce false information, bias, and toxic text outside the user's needs. One approach to address this issue is to prompt LLMs with task-specific instructions and/or solved examples, helping them learn patterns and perform a range of NLP tasks based on the user's expectations. In this setting a prompt can contain the following elements (See Figure 2):
- **Instruction**: Clearly defines the specific task the LLM should perform.
- **Context**: Offers additional context, including solved examples or supplementary information, to guide the model towards more contextually relevant responses.
- **Input Data**: Specifies the input data on which the task is to be performed. 
- **Output Indicator**: Articulates the desired format for the model's output.

It is important to note that while these elements can significantly enhance prompt effectiveness, they are not mandatory. Users have the flexibility to design and customize prompts according to their specific requirements and preferences.


[^1]: Liu, Pengfei, et al. "Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing." ACM Computing Surveys 55.9 (2023): 1-35.
[^2]: Brown, Tom, et al. "Language models are few-shot learners." Advances in neural information processing systems 33 (2020): 1877-1901.
[^3]: Touvron, Hugo, et al. "Llama 2: Open foundation and fine-tuned chat models." arXiv preprint arXiv:2307.09288 (2023).
