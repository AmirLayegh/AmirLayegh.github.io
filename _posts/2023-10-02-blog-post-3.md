---
title: 'Prompt Construction Guide'
date: 2023-10-09
permalink: /posts/2023/10/blog-post-3/
output: html_document
bibliography: refs.bib  
tags:
  - Prompt Construction
  - Prompt Engineering
  - Large Language Models
---

Prompt Engineering
======
Prompt engineering is a discipline focused on crafting efficient prompts for large language models (LLMs). These skills aid in comprehending LLM capabilities and limitations. Researchers apply prompt engineering to enhance LLM performance in various tasks, from QA to arithmetic reasoning. Developers use it to create compelling interfaces for LLMs and other tools.

Beyond prompt design, prompt engineering encompasses a spectrum of skills essential for LLM interaction and development. Prompt engineering is a crucial competency for understanding and enhancing LLM capabilities, improving safety, and incorporating domain knowledge and external tools.

This guide compiles the latest papers, and models for prompt engineering to meet the growing interest in LLM development. In this blog post, I delve into the art of prompt engineering for LLMs, exploring a variety of strategies and techniques to craft effective prompts that harness the full potential of these robust AI systems. Whether you are a developer, researcher, or enthusiast, this resource will empower you to create prompts that yield insightful and contextually relevant responses.

In this guide, we divide the prompts into two distinct settings: 
- [Fill-in Slot Prompts](#fill-in-slot-prompts-for-prompt-based-learning)
- [Prompts for Generative Large Language Models](#prompts-for-generative-large-language-models) 

This division helps us explore specialized techniques and best practices for each type of prompt construction, ensuring that you have a comprehensive understanding of how to effectively engineer prompts for both scenarios.

## Fill-in Slot Prompts for Prompt-based Learning

![Prompt learning](/images/prompt.png)
<figcaption> Figure 1: The example of prompt-based learning for sentiment classification.</figcaption>

In contrast to traditional supervised learning, which trains a model to take an input $x$ and predict an output $y$ as $P(y|x)$, **prompt-based learning** relies on language models that directly model the probability of text[^1]. In this paradigm, a textual prompt template with unfilled slots is applied to an input sentence $x$, transforming the downstream task into a Masked Language Modeling (MLM) problem. This reformulation empowers the language model to statistically fill in the unfilled slots, which can be considered as the predicted label $y$ for the specific task.
For instance the prompt template $T$ for a sentiment classification task can be defined as follow:
```
T = x. The sentiment of the sentence is [MASK].
```
where $x$ indicates the original input sentence.

In case of having an input sentence $x$ = *That is an awesome product.*, applying the prompt template $T$ on the input sentence $x$, we will have the prompted input sentence $x^ \prime$ as:
```
x´ = That is an awesome product. The sentiment of the sentence is [MASK].
```

Given the prompted input sentnece $x ^\prime$, the language model will fill the unfilled mask token [MASK], which will be translated into one of the actual labels $y$ by a verbalizer (See Figure 1).

## Prompts for Generative Large Language Models

![](/images/Prompt-LLM.png)
<figcaption> Figure 2: Example of prompting LLMs on sentiment classification task.</figcaption>

LLMs, such as GPT-3[^2] and Llama 2[^3], are usually very good at generating grammatically correct and semantically meaningful text. However, despite their outstanding performance, they can produce false information, bias, and toxic text outside the user's requirements. One approach to address this issue is to prompt LLMs with task-specific instructions and/or solved examples, helping them learn patterns and perform a range of NLP tasks based on the user's expectations. In this setting a prompt can contain the following elements (See Figure 2):
- **Instruction**: Clearly defines the specific task the LLM should perform.
- **Context**: Offers additional context, including solved examples or supplementary information, to guide the model towards more contextually relevant responses.
- **Input Data**: Specifies the input data on which the task is to be performed. 
- **Output Indicator**: Articulates the desired format for the model's output.

It is important to note that while these elements can significantly enhance prompt effectiveness, they are not mandatory. Users have the flexibility to design and customize prompts according to their specific requirements and preferences.


### Zero-Shot Prompting

As mentioned earlier, LLMs, trained on extensive data and finely tuned to follow instructions, can perform tasks with zero-shot ability. Zero-shot prompting involves providing a prompt that is not part of the training data to the model, yet the model can generate a result that the user desires. This promising technique further enhances the utility of LLMs for a wide range of tasks. Here is one example for zero-shot prompting:

![](/images/zero-shot.png)
<figcaption> Figure 3: Zero-shot prompting for sentiment classification.</figcaption>

As illustrated in Figure 3, the prompt lacks a solved example, which characterizes this prompting approach as *zero-shot prompting*.

### Few-Shot Prompting (In-Context Learning)

While LLMs already demonstrate impressive zero-shot capabilities, they still face challenges with more complex tasks in a zero-shot setting. To address this challenge, Few-shot prompting, or In-Context Learning (ICL), is introduced. ICL represents a distinct approach to prompt engineering, where task demonstrations are integrated into the prompt in natural language format. Through ICL, the capability of LLMs is leveraged to handle novel tasks without requiring fine-tuning. Furthermore, ICL offers the versatility to be combined with fine-tuning, unlocking even greater potential for more dynamic and robust LLMs. Here is one example for ICL:

![](/images/few-shot.png)
<figcaption> Figure 4: In-context learning example for sentiment classification.</figcaption>

As illusterated in Figure 4, there are some solved examples involved in the prompt to improve the response of LLM. This approach is called *few-shot prompting* which is also known as *in-context learning*.

## Manual Discrete Prompt Engineering

![](/images/manual-prompt.png)
<figcaption> Figure 5: Texts generated with different manually designed instructions for input x= "Dear John, Your Internet Banking accounts are now setup again for accessing. The login id is still your main account with the password being reset to the last six (6) digits of your SSN." As it can be seen, Without any instructions, the model simply generates a continuation of the given input (top). Providing an instruction makes it generate an appropriate summary (center) or e-mail title (bottom) even in zero-shot settings and enables much more data-efficient learning.</figcaption>

Manual construction based on human introspection is the most straightforward approach to creating a discrete (hard) prompt. In this context, a discrete prompt refers to one made up of individual tokens in natural language form. Prior work in this field includes the LAMA dataset[^4], which offers manually generated fill-in prompts for knowledge evaluation in language models. Additionally, Schick and Schütze[^5] [^6] [^7] utilize pre-defined prompts in a few-shot learning framework for tasks such as text classification and conditional text generation (See Figure 5 taken from Schick and Schütze[^6]).

## Automatic Prompt Engineering
Creating manual prompts may seem intuitive and offer some task-solving capabilities, but it has some challenges:
- Crafting and experimenting with these prompts can be time-consuming and requires expertise, especially for complex tasks like semantic parsing.
- Existing discrete prompt-based models, such as LAMA[^4] have shown that a single word change in prompts can cause an extreme difference in the results.
- Even with experienced prompt designers we may not end up with finding the most optimal prompt.
In light of these challenges, various methods have been introduced to automate prompt construction. These methods can be categorized into:
- Automatic discrete (hard) prompt engineering
- Automatic continous (soft) prompt engineering

In the following sections, we delve into the specifics of each category and explore the various proposed methods.

### Automatic Discrete Prompt Engineering

[AutoPrompt](https://arxiv.org/pdf/2010.15980.pdf) is an approach to automatically create the fill-in prompts for different NLP tasks based on the gradient search. They first add a number of trigger tokens shared across all prompts (denoted by [T] in Figure 6 taken from [AutoPrompt](https://arxiv.org/pdf/2010.15980.pdf)). These trigger tokens initialized as [MASK] tokens and then iteratively updated to maximize the label likelihood. At each step, they compute a first-order approximation of the change in the log-likelihood that would be produced by swapping the $j$th trigger token with another token in the language model vocabulary. Finally, after $k$ forward pass of the model, they select the top-$k$ tokens as candidates.

![](/images/AutoPrompt.png)
<figcaption> Figure 6: Illustration of AutoPrompt. Each input, $x_{inp}$, is placed into a natural language prompt, $x_{prompt}$, which contains a single [MASK] token. The prompt is created using a prompt template which combines the original input with a set of trigger tokens, $x_{trig}$. The trigger tokens are shared across all inputs and determined using a gradient-based search.</figcaption>

[Self-instruct](https://arxiv.org/pdf/2212.10560.pdf) is another framework proposed for automatic generation of optimal instructions that help language models improve their ability to follow natural language instructions. The process of self-instruct is an iterative bootstrapping algorithm that starts with a seed set of instructions that manually written by humans. These manually written instructions are used to generate new instructions with input-output pairs. Afterwards, the generated instructions are filetered out to remove low-quality or similar ones. The result of the self-instruct is a large collection of instructional data for different tasks and can be used to fine-tune LLMs on these different tasks (See Figure & taken from [Self-instruct](https://arxiv.org/pdf/2212.10560.pdf)).

![](/images/pipeline.JPG)
<Figcaption> Figure 7: A high-level overview of SELF-INSTRUCT. The process starts with a small seed set of tasks as the task pool. Random tasks are sampled from the task pool, and used to prompt an off-the-shelf LM to generate both new instructions and corresponding instances, followed by filtering low-quality or similar generations, and then added back to the initial repository of tasks. The resulting data can be used for the instruction tuning of the language model itself later to follow instructions better.</figcaption>

[Zhou et al.](https://arxiv.org/pdf/2211.01910.pdf) proposed an Automatic Prompt Engineer (APE) framework for automatic instruction generation and selection. This approach reformulates the instruction generation as natural language synthesis considered as a black box optimization problem using LLMs to generate and score them to choose candidate solutions. In the first step, this framework employs an LLM (as an inference model) that is given a set of input-output pairs to generate instruction candidates. Afterwards, another LLM (target model) is used to produce the log-likelihood of each generated instruction as its corresponding score. Finally, the most appropriate instruction is selected based on the scores (See Figure 8 taken from [APE](https://arxiv.org/pdf/2211.01910.pdf)). The main contribution of this paper is to automatically optimizing the prompts.

![](/images/APE.png)
<figcaption> Figure 8: An overview of APE framework.</figcaption>

[LM-BFF](https://arxiv.org/pdf/2012.15723.pdf) (better few-shot fine-tuning of language models) is another proposed prompt-based fine-tunig framework for few-shot NLP tasks including a novel pipeline for fill-in prompt generation. This generation pipeline dynamically incorporates the demonstrations into each prompt. As demonstrated in Figure 9 (taken from [LM-BFF](https://arxiv.org/pdf/2012.15723.pdf)), this framework uses **T5** language model to generate the prompt candidates given input sentences from training dataset concatenated with their corresponding label words. Here, the beam search is applied to select a wide range of decoded prompts. Afterwards, they fine-tune each generated prompt on training dataset and use evaluation dataset to select the most appropriate prompt with the best performance. When the optimal generated fill-in prompt is selected, inspired by in-context learning, during the second stage they sample similar examples for each label from the training dataset. By similar examples we refer to sentences that are semantically close to the input sentence (use SBERT to obtain embeddings). After selecting the semantically close sentences for each label, they concatenate these sentences with their labels (as demonstrations) to the input sentence (See Figure 10 taken from [LM-BFF](https://arxiv.org/pdf/2012.15723.pdf)).

![](/images/LM-BFF-Prompt.png)
<figcaption> Figure 9: LM-BFF approach for prompt generation and selection.</figcaption>

![](/images/LM-BFF-approach.png)
<figcaption> Figure 10: LM-BFF for prompt-based fine-tuning with demonstrations.</figcaption>

#### Incorporate Knowledge into Promopt
Another approach to automatically generate optimal prompts involves infusing knowledge into the prompt, enhancing the language model's predictive accuracy. In this context, [Liu et al.](https://arxiv.org/pdf/2110.08387.pdf) introduced a method known as generated knowledge prompting. This approach encompasses two stages:

- 1. **Knowledge Generation**: A language model generates knowledge statements based on the given question (P(k|q)).
- 2. **Knowledge Integration**: The generated knowledge statement is incorporated into the question (prompt) and presented to the language model as an inference model to predict the answer (Refer to Figure 11 in [Liu et al.](https://arxiv.org/pdf/2110.08387.pdf)). This process can be represented as follows:

$$a ^\prime = arg max P(a|q, K_{q})$$

where $q$ indicates the question, $K_{q}$ shows the corresponding knowledge statement, and $a^ \prime$ is the predicted answer. Table 1 (taken from [Liu et al.](https://arxiv.org/pdf/2110.08387.pdf)) demonstrates the examples for knowledge statement generation, and Table 2 (taken from [Liu et al.](https://arxiv.org/pdf/2110.08387.pdf)) indicates the generated answers by LLMs in two different settings where we (1) incorporate knowledge statements, and (2) **not** incorporating knowledge statements into the prompts (for more examples refer to [this post](https://www.promptingguide.ai/techniques/knowledge)).

![](/images/Knowledge-for-prompt.png)
<figcaption> Figure 11: Generated knowledge prompting involves (i) using few-shot demonstrations to generate questionrelated knowledge statements from a language model; (ii) using a second language model to make predictions with each knowledge statement, then selecting the highest-confidence prediction.</figcaption>

![](/images/knowledge-generation.png)
<figcaption> Table1: Examples for knowledge generation for two NLP tasks.</figcaption>

![](/images/Result-Knowledge-for-ptompt.png)
<figcaption> Table 2: Examples where prompting with generated knowledge rectifies model prediction. Each section shows the correct answer in green, the incorrect answer in red, and the prediction scores from the inference model that only sees the question (top) and the same model that sees the question prompted with the given knowledge (bottom).</figcaption>

[Guo et al.](https://openreview.net/pdf?id=9TdCcMlmsLm) introduced a new RL formulation for text generation from the soft Q-learning (SQL) perspective. Figure 12 (taken from [Guo et al.](https://openreview.net/pdf?id=9TdCcMlmsLm)) illustrates the method of Q-learning for text generation.

![](/images/Q-learning.png)
<figcaption> Figure 12: **Left**: An overview of the proposed SQL algorithm for text generation. Text generation is challenging due to sparse reward (i.e., the rewards of all intermediate steps are 0) and large action space (i.e., large vocabulary). **Right**: diverse applications of the text-generation RL algorithm.</figcaption>

### Automatic Continous Prompt Engineering


## Chain of Thought

## Retrieval Augmented Generation (RAG)


## Graph Prompting




[^1]: Liu, Pengfei, et al. "Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing." ACM Computing Surveys 55.9 (2023): 1-35.
[^2]: Brown, Tom, et al. "Language models are few-shot learners." Advances in neural information processing systems 33 (2020): 1877-1901.
[^3]: Touvron, Hugo, et al. "Llama 2: Open foundation and fine-tuned chat models." arXiv preprint arXiv:2307.09288 (2023).
[^4]: Petroni, Fabio, et al. "Language models as knowledge bases?." arXiv preprint arXiv:1909.01066 (2019).
[^5]: Schick, Timo, and Hinrich Schütze. "Exploiting cloze questions for few shot text classification and natural language inference." arXiv preprint arXiv:2001.07676 (2020).
[^6]: Schick, Timo, and Hinrich Schütze. "Few-shot text generation with natural language instructions." Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. 2021.
[^7]: Schick, Timo, and Hinrich Schütze. "It's not just size that matters: Small language models are also few-shot learners." arXiv preprint arXiv:2009.07118 (2020).