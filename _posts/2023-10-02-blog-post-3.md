---
title: 'Prompt Construction Guide'
date: 2023-10-09
permalink: /posts/2023/10/blog-post-3/
output: html_document
bibliography: refs.bib  
tags:
  - Prompt Construction
  - Prompt Engineering
  - Large Language Models
---

Prompt Engineering
======
Prompt engineering is a discipline focused on crafting efficient prompts for large language models (LLMs). These skills aid in comprehending LLM capabilities and limitations. Researchers apply prompt engineering to enhance LLM performance in various tasks, from QA to arithmetic reasoning. Developers use it to create compelling interfaces for LLMs and other tools.

Beyond prompt design, prompt engineering encompasses a spectrum of skills essential for LLM interaction and development. Prompt engineering is a crucial competency for understanding and enhancing LLM capabilities, improving safety, and incorporating domain knowledge and external tools.

This guide compiles the latest papers, and models for prompt engineering to meet the growing interest in LLM development. In this blog post, I delve into the art of prompt engineering for LLMs, exploring a variety of strategies and techniques to craft effective prompts that harness the full potential of these robust AI systems. Whether you are a developer, researcher, or enthusiast, this resource will empower you to create prompts that yield insightful and contextually relevant responses.

In this guide, we divide the prompts into two distinct settings: 
- [Fill-in Slot Prompts](#fill-in-slot-prompts-for-prompt-based-learning)
- [Prompts for Generative Large Language Models](#prompts-for-generative-large-language-models) 

This division helps us explore specialized techniques and best practices for each type of prompt construction, ensuring that you have a comprehensive understanding of how to effectively engineer prompts for both scenarios.

## Fill-in Slot Prompts for Prompt-based Learning

![Prompt learning](/images/prompt.png)
<figcaption> Figure 1: The example of prompt-based learning for sentiment classification.</figcaption>

In contrast to traditional supervised learning, which trains a model to take an input $x$ and predict an output $y$ as $P(y|x)$, **prompt-based learning** relies on language models that directly model the probability of text[^1]. In this paradigm, a textual prompt template with unfilled slots is applied to an input sentence $x$, transforming the downstream task into a Masked Language Modeling (MLM) problem. This reformulation empowers the language model to statistically fill in the unfilled slots, which can be considered as the predicted label $y$ for the specific task.
For instance the prompt template $T$ for a sentiment classification task can be defined as follow:
```
T = x. The sentiment of the sentence is [MASK].
```
where $x$ indicates the original input sentence.

In case of having an input sentence $x$ = *That is an awesome product.*, applying the prompt template $T$ on the input sentence $x$, we will have the prompted input sentence $x^ \prime$ as:
```
x´ = That is an awesome product. The sentiment of the sentence is [MASK].
```

Given the prompted input sentnece $x ^\prime$, the language model will fill the unfilled mask token [MASK], which will be translated into one of the actual labels $y$ by a verbalizer (See Figure 1).

## Prompts for Generative Large Language Models

![](/images/Prompt-LLM.png)
<figcaption> Figure 2: Example of prompting LLMs on sentiment classification task.</figcaption>

LLMs, such as GPT-3[^2] and Llama 2[^3], are usually very good at generating grammatically correct and semantically meaningful text. However, despite their outstanding performance, they can produce false information, bias, and toxic text outside the user's requirements. One approach to address this issue is to prompt LLMs with task-specific instructions and/or solved examples, helping them learn patterns and perform a range of NLP tasks based on the user's expectations. In this setting a prompt can contain the following elements (See Figure 2):
- **Instruction**: Clearly defines the specific task the LLM should perform.
- **Context**: Offers additional context, including solved examples or supplementary information, to guide the model towards more contextually relevant responses.
- **Input Data**: Specifies the input data on which the task is to be performed. 
- **Output Indicator**: Articulates the desired format for the model's output.

It is important to note that while these elements can significantly enhance prompt effectiveness, they are not mandatory. Users have the flexibility to design and customize prompts according to their specific requirements and preferences.


### Zero-Shot Prompting

As mentioned earlier, LLMs, trained on extensive data and finely tuned to follow instructions, can perform tasks with zero-shot ability. Zero-shot prompting involves providing a prompt that is not part of the training data to the model, yet the model can generate a result that the user desires. This promising technique further enhances the utility of LLMs for a wide range of tasks. Here is one example for zero-shot prompting:

![](/images/zero-shot.png)
<figcaption> Figure 3: Zero-shot prompting for sentiment classification.</figcaption>

As illustrated in Figure 3, the prompt lacks a solved example, which characterizes this prompting approach as *zero-shot prompting*.

### Few-Shot Prompting (In-Context Learning)

While LLMs already demonstrate impressive zero-shot capabilities, they still face challenges with more complex tasks in a zero-shot setting. To address this challenge, Few-shot prompting, or In-Context Learning (ICL), is introduced. ICL represents a distinct approach to prompt engineering, where task demonstrations are integrated into the prompt in natural language format. Through ICL, the capability of LLMs is leveraged to handle novel tasks without requiring fine-tuning. Furthermore, ICL offers the versatility to be combined with fine-tuning, unlocking even greater potential for more dynamic and robust LLMs. Here is one example for ICL:

![](/images/few-shot.png)
<figcaption> Figure 4: In-context learning example for sentiment classification.</figcaption>

As illusterated in Figure 4, there are some solved examples involved in the prompt to improve the response of LLM. This approach is called *few-shot prompting* which is also known as *in-context learning*.

## Manual Discrete Prompt Engineering

![](/images/manual-prompt.png)
<figcaption> Figure 5: Texts generated with different manually designed instructions for input x= "Dear John, Your Internet Banking accounts are now setup again for accessing. The login id is still your main account with the password being reset to the last six (6) digits of your SSN." As it can be seen, Without any instructions, the model simply generates a continuation of the given input (top). Providing an instruction makes it generate an appropriate summary (center) or e-mail title (bottom) even in zero-shot settings and enables much more data-efficient learning.</figcaption>

Manual construction based on human introspection is the most straightforward approach to creating a discrete (hard) prompt. In this context, a discrete prompt refers to one made up of individual tokens in natural language form. Prior work in this field includes the LAMA dataset[^4], which offers manually generated fill-in prompts for knowledge evaluation in language models. Additionally, Schick and Schütze[^5] [^6] [^7] utilize pre-defined prompts in a few-shot learning framework for tasks such as text classification and conditional text generation (See Figure 5 taken from Schick and Schütze[^6]).

## Automatic Prompt Engineering
Creating manual prompts may seem intuitive and offer some task-solving capabilities, but it has some challenges:
- Crafting and experimenting with these prompts can be time-consuming and requires expertise, especially for complex tasks like semantic parsing.
- Existing discrete prompt-based models, such as LAMA[^4] have shown that a single word change in prompts can cause an extreme difference in the results.
- Even with experienced prompt designers we may not end up with finding the most optimal prompt.
In light of these challenges, various methods have been introduced to automate prompt construction. These methods can be categorized into:
- Automatic discrete (hard) prompt engineering
- Automatic continous (soft) prompt engineering

In the following sections, we delve into the specifics of each category and explore the various proposed methods.

### Automatic Discrete Prompt Engineering

[AutoPrompt](https://arxiv.org/pdf/2010.15980.pdf) is an approach to automatically create the fill-in prompts for different NLP tasks based on the gradient search. They first add a number of trigger tokens shared across all prompts (denoted by [T] in Figure 6 taken from [AutoPrompt](https://arxiv.org/pdf/2010.15980.pdf)). These trigger tokens initialized as [MASK] tokens and then iteratively updated to maximize the label likelihood. At each step, they compute a first-order approximation of the change in the log-likelihood that would be produced by swapping the $j$th trigger token with another token in the language model vocabulary. Finally, after $k$ forward pass of the model, they select the top-$k$ tokens as candidates.

![](/images/AutoPrompt.png)
<figcaption> Figure 6: Illustration of AutoPrompt. Each input, $x_{inp}$, is placed into a natural language prompt, $x_{prompt}$, which contains a single [MASK] token. The prompt is created using a prompt template which combines the original input with a set of trigger tokens, $x_{trig}$. The trigger tokens are shared across all inputs and determined using a gradient-based search.</figcaption>

[Self-instruct](https://arxiv.org/pdf/2212.10560.pdf) is another framework proposed for automatic generation of optimal instructions that help language models improve their ability to follow natural language instructions. The process of self-instruct is an iterative bootstrapping algorithm that starts with a seed set of instructions that manually written by humans. These manually written instructions are used to generate new instructions with input-output pairs. Afterwards, the generated instructions are filetered out to remove low-quality or similar ones. The result of the self-instruct is a large collection of instructional data for different tasks and can be used to fine-tune LLMs on these different tasks (See Figure & taken from [Self-instruct](https://arxiv.org/pdf/2212.10560.pdf)).

![](/images/pipeline.JPG)
<Figcaption> Figure 7: A high-level overview of SELF-INSTRUCT. The process starts with a small seed set of tasks as the task pool. Random tasks are sampled from the task pool, and used to prompt an off-the-shelf LM to generate both new instructions and corresponding instances, followed by filtering low-quality or similar generations, and then added back to the initial repository of tasks. The resulting data can be used for the instruction tuning of the language model itself later to follow instructions better.</figcaption>

[Zhou et al.](https://arxiv.org/pdf/2211.01910.pdf)

[LM-BFF](https://arxiv.org/pdf/2012.15723.pdf)

[RL-for generation](https://openreview.net/pdf?id=9TdCcMlmsLm)

[Generated Knowledge Prompting for Commonsense Reasoning](https://arxiv.org/pdf/2110.08387.pdf)

### Automatic Continous Prompt Engineering


### Chain of Thought

### Retrieval Augmented Generation (RAG)


### Graph Prompting




[^1]: Liu, Pengfei, et al. "Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing." ACM Computing Surveys 55.9 (2023): 1-35.
[^2]: Brown, Tom, et al. "Language models are few-shot learners." Advances in neural information processing systems 33 (2020): 1877-1901.
[^3]: Touvron, Hugo, et al. "Llama 2: Open foundation and fine-tuned chat models." arXiv preprint arXiv:2307.09288 (2023).
[^4]: Petroni, Fabio, et al. "Language models as knowledge bases?." arXiv preprint arXiv:1909.01066 (2019).
[^5]: Schick, Timo, and Hinrich Schütze. "Exploiting cloze questions for few shot text classification and natural language inference." arXiv preprint arXiv:2001.07676 (2020).
[^6]: Schick, Timo, and Hinrich Schütze. "Few-shot text generation with natural language instructions." Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. 2021.
[^7]: Schick, Timo, and Hinrich Schütze. "It's not just size that matters: Small language models are also few-shot learners." arXiv preprint arXiv:2009.07118 (2020).