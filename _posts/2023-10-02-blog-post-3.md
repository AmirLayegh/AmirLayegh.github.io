---
title: 'Prompt Construction Guide'
date: 2023-10-09
permalink: /posts/2023/10/blog-post-3/
output: html_document
bibliography: refs.bib  
tags:
  - Prompt Construction
  - Prompt Engineering
  - Large Language Models
---

Prompt Engineering
======
Prompt engineering is a discipline focused on crafting efficient prompts for large language models (LMs). These skills aid in comprehending LLM capabilities and limitations. Researchers apply prompt engineering to enhance LLM performance in various tasks, from QA to arithmetic reasoning. Developers use it to create compelling interfaces for LLMs and other tools.

Beyond prompt design, prompt engineering encompasses a spectrum of skills essential for LLM interaction and development. It is a crucial competency for understanding and enhancing LLM capabilities, improving safety, and incorporating domain knowledge and external tools.

This guide compiles the latest papers, models, and tools pertinent to prompt engineering to meet the growing interest in LLM development. In this blog post, we delve into the art of prompt engineering for large language models, exploring a variety of strategies and techniques to craft effective prompts that harness the full potential of these robust AI systems. Whether you are a developer, researcher, or enthusiast, this resource will empower you to create prompts that yield insightful and contextually relevant responses.

In this guide, we divide the prompts into two distinct settings: *Fill-in Slot Prompts* and *Prompts for Generative Large Language Models.* This division will help us explore specialized techniques and best practices for each type of prompt construction, ensuring that you have a comprehensive understanding of how to effectively engineer prompts for both scenarios.

## Fill-in Slot Prompts for Prompt-based Learning
In contrast to traditional supervised learning, which trains a model to take an input $x$ and predict an output $y$ as $P(y|x)$, **prompt-based learning** relies on language models that directly model the probability of text[^1]. In this paradigm, a textual prompt template with unfilled slots is applied to an input sentence $x$, transforming the downstream task into a Masked Language Modeling (MLM) problem. This reformulation empowers the language model to statistically fill in the unfilled slots, which can be considered as the predicted label $y$ for the specific task.
For instance the prompt template $\mathcal{T}$ for a sentiment classification task can be defined as follow:

$\mathcal{T}$ = $x$. The sentiment of the sentence is [MASK].

where $x$ indicates the original input sentence.

In case of having an input sentence $x$ = *That is an awesome product.*, applying the prompt template $\mathcal{T}$ on the input sentence $x$, we will have the prompted input sentence $x^ \prime$ as:

$x^ \prime$ = That is an awesome product. The sentiment of the sentence is [MASK].

Given the prompted input sentnece $x ^\prime$, the language model will predict the mask token [MASK], which will be translated into an actual label $y$ by a verbalizer (See Figure 1).

![Prompt learning](/images/prompt.png)
<figcaption> The example of prompt-based learning for sentiment analysis.</figcaption>

## Prompts for Generative Large Language Models


[^1]: Liu, Pengfei, et al. "Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing." ACM Computing Surveys 55.9 (2023): 1-35.
