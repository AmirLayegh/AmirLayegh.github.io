---
title: 'Hands on Transformers'
date: 2024-01-15
permalink: /posts/2023/10/blog-post-4/
tags:
  - Transformers
  - NLP
  - Attention
---

Transformer Anatomy
=====
[](/images/transformers.png)
<figcaption> Figure 1: Transformer encoder-decoder architectture.</figcaption>

Transformers represent a groundbreaking paradigm shift in artificial intelligence and natural language processing. The original transformer is based on the encoder-decoder architecture that is used for different tasks such as machine translation, named entity recognition, text summarization and suchlike.

Encoder
===
The encoder converts an input sequence of tokens into a sequence of embedding vecotrs, so called *hidden state* or *context*.

Decoder
===
Use the encoder's hidden states to iteratively generate an output sequence of tokens, one token at a time.

