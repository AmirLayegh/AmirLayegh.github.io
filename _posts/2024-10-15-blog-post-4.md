---
title: 'Hands on Transformers'
date: 2024-01-15
permalink: /posts/2023/10/blog-post-4/
tags:
  - Transformers
  - NLP
  - Attention
---

Transformer Anatomy
=====
Transformers represent a groundbreaking paradigm shift in artificial intelligence and natural language processing. The original transformer is based on the encoder-decoder architecture that is used for different tasks such as machine translation, named entity recognition, text summarization and suchlike.

![](/images/transformers.png)
<figcaption> Figure 1: Transformer encoder-decoder architecture.</figcaption>

Encoder
===
The encoder consists of many encoder stacked next to each other. The encoder converts an input sequence of tokens into a sequence of embedding vecotrs, so called *hidden state* or *context*.
Each encoder layer recieves a sequence of embeddings and feed them through the following sublayers see [Figure 2](/images/encoder.png):
- A **multi-head attention** layer
- A fully connected **feed-forward** layer that is applied to each input embedding

![](/images/encoder.png)
<figcaption> Figure 2: Encoder layer </figcaption>


Decoder
===
Use the encoder's hidden states to iteratively generate an output sequence of tokens, one token at a time.

Self-Attention
===
The attention is a mehcanism that allows neural networks to assign a different weight to each token in the sequence. 
The main idea behind the **self-attention** is that instead of using a fixed-embedding for each token, we can use the whole sequence to compute a *weighted-average* of each embedding. Given a sequence of token embeddings $x_1, x_2, ..., x_n$ self-attention produces a sequence of new embeddings $x'_1, x'_2, ..., x'_n$ where each $x'_i$ is a **linear combination** of all the $x_j$:

$$x'_i = \sum w_{ij}x_{j} $$

The coefficients $w_{ij}$, referred to as *attention weights*, play a pivotal role in self-attention mechanisms. To grasp the concept of self-attention, consider the word "flies." Initially, one might associate it with the small insects. However, with additional context such as "time flies like an arrow," a new understanding emerges, revealing that "flies" functions as a verb. Through the efficacy of self-attention, we can assign greater weights $w_{ij}$ to the embeddings of tokens "time" and "arrow," effectively incorporating the context. This allows the model to prioritize relevant information, demonstrating the power of self-attention in capturing intricate relationships within the sequences.


