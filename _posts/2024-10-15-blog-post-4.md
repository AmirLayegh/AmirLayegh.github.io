---
title: 'Hands on Transformers'
date: 2024-01-15
permalink: /posts/2023/10/blog-post-4/
tags:
  - Transformers
  - NLP
  - Attention
---

Transformer Anatomy
=====
Transformers represent a groundbreaking paradigm shift in artificial intelligence and natural language processing. The original transformer is based on the encoder-decoder architecture that is used for different tasks such as machine translation, named entity recognition, text summarization and suchlike.

![](/images/transformers.png)
<figcaption> Figure 1: Transformer encoder-decoder architecture.</figcaption>

Encoder
===
The encoder consists of many encoder stacked next to each other. The encoder converts an input sequence of tokens into a sequence of embedding vecotrs, so called *hidden state* or *context*.
Each encoder layer recieves a sequence of embeddings and feed them through the following sublayers see [Figure 2](/images/encoder.png):
- A **multi-head attention** layer
- A fully connected **feed-forward** layer that is applied to each input embedding

![](/images/encoder.png)
<figcaption> Figure 2: Encoder layer </figcaption>


Decoder
===
Use the encoder's hidden states to iteratively generate an output sequence of tokens, one token at a time.

Self-Attention
===
The attention is a mehcanism that allows neural networks to assign a different weight to each token in the sequence. 
The main idea behind the **self-attention** is that instead of using a fixed-embedding for each token, we can use the whole sequence to compute a *weighted-average* of each embedding. Given a sequence of token embeddings $x_1, x_2, ..., x_n$ self-attention produces a sequence of new embeddings $x'_1, x'_2, ..., x'_n$ where each $x'_i$ is a **linear combination** of all the $x_j$:

$$x'_i = \sum w_{ij}x_{j} $$

The coefficients $w_{ij}$, referred to as *attention weights*, play a pivotal role in self-attention mechanisms. To grasp the concept of self-attention, consider the word "flies." Initially, one might associate it with the small insects. However, with additional context such as "time flies like an arrow," a new understanding emerges, revealing that "flies" functions as a verb. Through the efficacy of self-attention, we can assign greater weights $w_{ij}$ to the embeddings of tokens "time" and "arrow," effectively incorporating the context. This allows the model to prioritize relevant information, demonstrating the power of self-attention in capturing intricate relationships within the sequences. A diagram of process is show in [Figure 3](/images/self-attention.png).

![](/images/self-attention.png)
<figcaption> Figure 3: Self-attention mechanism example. The upper section shows raw token embeddings and the lower is the contextualized embeddings produced by self-attention.</figcaption>

Attention Weights Calculation
===
The most common way to calculate the self-attention weights is to use *scaled dot-product*. 
The following four steps are required to implement this mechanism:
- Create three vectors called *query*, *key*, *value* for each token embedding.
- Compute attention scores. At this stage we need to determine the similarity between the query, and key vectors. The similarity function which is used here is the **dot-product**. Queries and keys that are similar will have a large dot product, while those with lower similarity will have less or zero dot product. The output from this step is called the *attention scores* and for a sequence of $n$ tokens we will have a matrix of $n \times n$ as attention score matrix.
- Compute attention weights by scaling and normalizing dot products. At this step we apply a scaling factor to stabilize training by normalizing variance. Afterwards, they will be normalized with a softmax to ensure all the column values sum to 1. The resulting $n \times n$ matrix now contains all the attention weights, w_{ji} 
- Update the token embeddings. Once the attention weights are cmputed, we multiply them by the value vector $v_1, v_2, ..., v_n$ to update the representation for embedding $x'_i = \sum w{ji} v_j$.
