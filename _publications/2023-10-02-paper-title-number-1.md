---
title: "ContrastNER: Contrastive-based Prompt Tuning for Few-shot NER"
collection: publications
permalink: /publication/2023-10-02-paper-title-number-1
excerpt: 'This paper is about the number 1. The number 2 is left for future work.'
date: 2023-10-02
venue: 'COMPSAC2023'
paperurl: 'https://www.computer.org/csdl/proceedings-article/compsac/2023/269700a241/1PhDNGXJoMU'
citation: 'Amirhossein Layegh, Layegh et al. (2022). &quot;ContrastNER: Contrastive-based Prompt Tuning for Few-shot NER.&quot; <i>COMPSAC2023</i>. 1(1).'
---
Prompt-based language models have produced encouraging results in numerous applications, including Named Entity Recognition (NER) tasks. NER aims to identify entities in a sentence and provide their types. However, the strong performance of most available NER approaches is heavily dependent on the design of discrete prompts and a verbalizer to map the model-predicted outputs to entity categories, which are complicated undertakings. To address these challenges, we present ContrastNER, a prompt-based NER framework that employs both discrete and continuous tokens in prompts and uses a contrastive learning approach to learn the continuous prompts and forecast entity types. The experimental results demonstrate that ContrastNER obtains competitive performance to the state-of-the-art NER methods in high-resource settings and outperforms the state-of-the-art models in low-resource circumstances without requiring extensive manual prompt engineering and verbalizer design.

[Download paper here](https://www.computer.org/csdl/proceedings-article/compsac/2023/269700a241/1PhDNGXJoMU)

Recommended citation: Amirhossein Layeghe, Layegh et al. (2023). "ContrastNER: Contrastive-based Prompt Tuning for Few-shot NER." <i>COMPSAC2023</i>. 1(1).